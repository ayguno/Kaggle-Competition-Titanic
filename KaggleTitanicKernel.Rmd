---
title: "Feature engineering and building classifiers to predict survival in Titanic"
author: "Ozan Aygun"
date: "5/16/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---
# Introduction and summary

This is my first journey on a Kaggle data set. I will try to explore the Titanic data set by using simple, parsimonious linear models, then perform feature engineering, missing value imputation and dimension reduction. Finally, I train classifiers by using random forest, support vector machines, gradient boosted trees algorithms. I will also explore stacking these classifiers and model tuning to improve accuracy of the predictions for survival. 

As a life long learner, I highly appreciate comments, suggestions and feedback on my approaches from the Kaggle community. Thanks for reading my Kernel and have a great time here!
___

# Loading and partitioning the data
```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```

```{r,echo=FALSE}
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Competition-Titanic")
```

Load the data sets:

```{r}
training <- read.csv("train.csv", stringsAsFactors = FALSE, na.strings = "")
testing <- read.csv("test.csv", stringsAsFactors = FALSE,na.strings = "")
```

Random partition of the training set into:

- training: train different models
- tune.test.set: initial out of the box performance

```{r,results='markup'}
library(caret)
set.seed(1234)
InTrain <- createDataPartition(y=training$Survived,p = 0.7,list = FALSE)
tune.test.set <- training[-InTrain,]
training <- training[InTrain,]
```

Summarize the training data set:

```{r}
summary(training)
table(training$Sex)
table(training$Embarked)
table(training$Survived)
```

The data is imbalanced in terms of gender as well as the outcome class. However, this would not prevent us from developing classifiers. It will probably make it harder though!

___

# Developing expectations from the data: basic exploratory data analysis

Let's start developing expectations from the data. Note that we will only use the training data set for exploration and feature engineering. This is important to avoid over-fitting and decreasing the bias in our final out-of the sample accuracy. 

___

## Relationships between Age and Family size
```{r}
#Exploration by pairs plot:

pairs(Survived ~ Age+ SibSp+Parch+Fare,pch =19, cex = 0.4,data=training,
      col= ifelse(training$Survived == 1, "navy","red"))

```
Without concluding too much from this plot, we can immediately start noticing certain relationships. 

- Age and SibSp, and Age and Parch are inversely related. This is intuitive as older individuals are expected to have larger family sizes.
- There seems to be a relationship between Age, Fare (and perhaps Pclass) and Survival. We will explore this further.

___

## The survival probability of lower fare class passengers is significantly lower than 1st class passengers  

An intuitive possibility is that more passengers who travel in the higher cabin classes might be survived. 

```{r}
table(training$Survived,training$Pclass)
```

We see that clearly less people survived from the 2nd and 3rd classes. We can go ahead and test if that difference is statistically significant. Fitting a logistic regression model between these two variables proves that indeed, **the probability of survival is significantly low in 2nd class compared to 1st class, and it is also significantly lower in 3rd class, compared to 2nd and 1st class passengers (p < 0.001).**

```{r}
summary(glm(factor(Survived) ~ factor(Pclass), data = training, family = "binomial"))
```

Therefore, the Pclass feature is an important one to keep in our data set.

___

## Females have statistically significant higher probability of survival

In the event of crisis, it is expected that women and children will get the priority in the rescue efforts. We indeed notice that more females were survived, despite their small proportion.
```{r}
table(training$Survived,training$Sex)
```

We can also investigate if this difference is statistically significant by fitting a logistic regression model. The beauty of logistic regression is that we can tailor our scientific question by including potential confounders into the model. In this case we suspect that Age could be a confounder for the observed relationship between Sex and Survival. We also add the interaction term to see if the effect of Sex is modified by Age as well.

```{r}
summary(glm(Survived ~  Age*Sex, data = training, family = "binomial"))
```

Fascinating! First, we notice that indeed males have significantly lower probability of survival (p < 0.01) even after adjusting for Age and the interaction between Sex and Age. It is also worth to note that there is statistically significant interaction between Age and Sex predictors (p < 0.05). 

**Therefore, sex is another feature we will definitely keep to train our classifiers.**

___

## Fare variable: does it explain any variability beyond Pclass? 

As we expected from the PClass - Survival relationship, passengers who paid higher are mostly those who survived:

```{r}

qplot(y = log(Fare), x = factor(Survived), data = training, geom = "boxplot", fill = factor(Survived))+theme_bw()+scale_fill_manual(values = c("red","blue"))

```

The question is: does Fare feature explain any variability beyond the Pclass? If not, we should drop this feature since we don't need two collinear features when training our classifiers. This will make our estimates biased.

When we fit Pclass together with Fare:
```{r}
summary(glm(Survived ~ factor(Pclass) + Fare, data = training, family = "binomial"))
```

Interestingly, we notice that Fare still remains significant (p < 0.05) in the presence of the Pclass feature. This implies that there is additional variability in the Survival outcome that is explained by Fare, perhaps because this is a continuous variable and Pclass is categorical. Therefore, it seems feasible that they explain some non-overlapping variance.

We will also keep Fare feature to train our model.

___

# Feature Engineering

At this stage we will continue to explore the training data set, with the difference that we will try to extract/engineer different features that are not immediately available to us. We will perform preliminary analyses to test the potential utility of these features. 

As always, we will perform all feature engineering, imputation and dimension reduction in the training data set and will apply into the test data sets exactly in the same way.

___

## Generate categorical variables

It is more useful to model a few features as categorical variables:

```{r}
training <- transform(training,Survived = factor(Survived), Pclass = factor(Pclass),
          Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))
tune.test.set <- transform(tune.test.set,Survived = factor(Survived), Pclass = factor(Pclass),Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))

testing <- transform(testing, Pclass = factor(Pclass),
          Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))

```

___

## Remove the passenger ID column from the training sets

This feature has no classification power and will generate bias in our classifiers:

```{r}
library(dplyr)
training <- dplyr::select(training,-PassengerId)
tune.test.set <- dplyr::select(tune.test.set,-PassengerId)
```

___

## Investigate the missing values: explore opportunities for imputation

```{r}
training.na <- as.data.frame(is.na(training));names(training.na) <- names(training)
apply(training.na,2,sum)
```

We notice that missing values appear in 3 variables; Age, Cabin and Embarked.

___

### Cabin feature: more than 75% missing data

From the available observations, we can infer that the first letter actually presents the cabin section. Let's extract the first letter from the available ones and test if they have any predictive value:

```{r}
library(ggplot2); library(dplyr)
Cabin.letter <-substr(training$Cabin[!training.na$Cabin],1,1)
Cabin.survival <- training$Survived[!training.na$Cabin]
Cabin.Pclass <- training$Pclass[!training.na$Cabin]
qplot(x = factor(Cabin.letter), fill = Cabin.survival)+scale_fill_manual(values = c("red","navy"))+theme_bw()
```

We notice that this feature might have some predictive value. Let's check if the cabin relates with the Pclass:

```{r}
qplot(x = factor(Cabin.letter), fill = Cabin.Pclass)+scale_fill_manual(values = c("red","navy","green"))+theme_bw()
```

We indeed notice that Cabin letters A,B,C are absolutely first class and therefore can be inferred from the Pclass variable. D and E are also more likely to be in 1st class. Gs are all coming from the 3rd Class. 

We can also check if fare relates with the cabin letter:
```{r}
Cabin.Fare <- training$Fare[!training.na$Cabin]
qplot(x = factor(Cabin.letter), y=Cabin.Fare, color = Cabin.survival)+scale_color_manual(values = c("red","navy"))+theme_bw()
```
The relationship between Fare and Cabin Letter is not so dramatic to allow some imputation. 

**The problem with imputing Cabin feature from Pclass is that we don't know whether the missing variables are MCAR (Missing Completely at Random) or whether there is a relationship between the reason they are missing and the outcome (Survival).**

Therefore, it would be more sensible to drop this variable and don't use in building our classifiers.

Remove Cabin feature from all data sets:

```{r}
library(dplyr)
training <- dplyr::select(training, - Cabin)
tune.test.set <- dplyr::select(tune.test.set, -Cabin)
testing <- dplyr::select(testing, -Cabin)
```

___

### Age feature: imputing missing data by using a multivariate linear model

We recall the relationship between Age and family size from our earlier pairs plot. Therefore, one intuitive imputation potential would be comparing the Age with SibSp feature:

```{r}
qplot(x = SibSp, y = Age, color = Survived, data = training)+
       theme_bw()+scale_color_manual(values = c("red","navy"))+theme_bw()
  
```

We notice that as the SibSp increases, the age group decreases. Let's look at the distribution of Age across the SibSp bins. 

```{r,fig.width=9}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ SibSp)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
```

Aha! Most of the Age distribution is having either no Spouses or siblings or only 1.  For the 1 siblings/spouses group **the mean age seems to be higher.** In both cases Age is approximated by normal distribution. The problem with SibSp is that some of the factor levels are only present in the observations where Age is missing, making this predictor unbalanced across the missing and complete cases.

It would be also interesting to explore gender differences when considering Age:

```{r, fig.width=9}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ Sex)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
```

How about the Age distribution in different passenger classes? (Pclass):

```{r,fig.width=9}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ Pclass)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
as.data.frame(training %>% group_by(Pclass,Survived) %>% summarise(mean.Age = mean(Age,na.rm=T)))
```

This is quite interesting! In all passenger classes, the mean age of the survived passengers is lower than those passed away. We also notice that the mean age decreases and the Class number increases, i.e: older passengers are in the better classes on average. Therefore, Pclass would also be include in the imputation model.

**It would be therefore sensible to impute Age by random gaussian imputation using the mean and standard deviation of individual factor levels of these predictors. We will include Sex in this model to account for gender-specific differences in Age, as well as Pclass to account for the interesting seperation of Age by Passenger Class we noted above.**

In order to do this more rigorously, we can first fit a linear model with the existing Age , Sex  and Pclass  data:

#### Fitting multivariate Age imputation model

The model will become:

Age ~  Sex + Pclass + e (random Gaussian error)

```{r,fig.width=9,fig.height=8}
lmAge = lm(Age ~ Sex + Pclass, data = training, na.action = "na.omit")
summary(lmAge)
par(mfrow = c(2,2))
plot(lmAge)[1:4]
```

This model is just OK, but we don't need the perfect model for this type of imputation. Nice to see that both variance and normality assumptions of the model holds and all levels of the covariates have significant impact on the mean outcome in the presence of each other. This would give us a good estimation for the missing values of Age.

Just to check if our imputation model yields the similar distribution as the original age:

```{r,fig.width=9}
complete.cases.Age <- training$Age[complete.cases(training$Age)]
after.imputation.Age <-c(complete.cases.Age,predict(lmAge, newdata = training[is.na(training$Age),]))
just.imputed.observations.Age <- after.imputation.Age[!complete.cases(training$Age)]

par(mfrow = c(1,3))
hist(complete.cases.Age,breaks = 20,col = "navy");
hist(after.imputation.Age,breaks = 20,col = "lightgreen");
hist(just.imputed.observations.Age,breaks = 20, col = "purple")
```

Therefore, our imputation model performs a nice job!

Using the model to impute missing values of Age:

```{r}
training$Age[is.na(training$Age)] = predict(lmAge, newdata = training[is.na(training$Age),])
tune.test.set$Age[is.na(tune.test.set$Age)] = predict(lmAge, newdata = tune.test.set[is.na(tune.test.set$Age),])
testing$Age[is.na(testing$Age)] = predict(lmAge, newdata = testing[is.na(testing$Age),])
```

Note that we use the same model object we derived from the training data set to impute the missing values of Age for all data sets. This will prevent us from over-fitting to the test data set.

Re-investigate the missing values:

```{r}
training.na <- as.data.frame(is.na(training));names(training.na) <- names(training)
tune.testing.na <- as.data.frame(is.na(tune.test.set));names(tune.testing.na) <- names(tune.test.set)
testing.na <- as.data.frame(is.na(testing));names(testing.na) <- names(testing)
data.frame(apply(training.na,2,sum),apply(tune.testing.na ,2,sum),apply(testing.na ,2,sum))
```

It appears that only one case is left missing in each of the data sets. We will consider this as random missingness and remove in each data set:

```{r}
training <- training[complete.cases(training),]
tune.test.set <- tune.test.set[complete.cases(tune.test.set),]
testing <- testing[complete.cases(testing),]
```

Now we have completed processing the missing values.

## Generate Dummy Variables with Categorical variables

Before we start building our classifiers, it is desirable to convert the categorical variables to dummy variables (0 or 1). This will preserve the parsimony if later we need to interpret the results of any decision trees. Furthermore, many machine learning packages work well with dummy variables.

```{r}
library(caret)
which(sapply(training[,-1],is.factor))
# There are 5 categorical predictors in out data set
factors.training <- which(sapply(training,is.factor))
factors.tune.test.set <- which(sapply(tune.test.set,is.factor))
factors.testing <- which(sapply(testing,is.factor))

dummies.training <- dummyVars(Survived ~ Pclass + Sex + SibSp + Parch + Embarked, data = training)
dummies.tune.test.set <- dummyVars(Survived ~ Pclass + Sex + SibSp + Parch + Embarked, data = tune.test.set)
dummies.testing <- dummyVars(PassengerId ~ Pclass + Sex + SibSp + Parch + Embarked, data = testing)

# Add the dummy variables to both training and test data sets, simultaneously removing the existing factor variables:
training <- cbind(training[,-factors.training[-1]], predict(dummies.training,newdata = training))
tune.test.set <- cbind(tune.test.set[,-factors.tune.test.set[-1]], predict(dummies.tune.test.set,newdata = tune.test.set))
testing <- cbind(testing[,-factors.testing], predict(dummies.testing,newdata = testing))
```

That was lots of code! Still, using caret package makes the process less cumbersome. 

After doing that we notice that there are differences in the factor levels of certain variables are not present in the training set and test sets. These are present in Parch and SibSp features. The lower levels of these variables are conserved in all sets and they represent most of the data. At this stage we will only keep features that are present in all sets:

```{r}
training <- training[,names(training) %in% names(tune.test.set)]
tune.test.set <- tune.test.set[,names(tune.test.set) %in% names(training)]
identical(names(training),names(tune.test.set))
PassengerId <- testing$PassengerId
testing <- testing[,names(testing) %in% names(training)]
testing$PassengerId <- PassengerId
```

___

## Feature engineering with Name and Ticket variables

### Ticket feature: sub-clusters with classification value

It would be interesting to just convert the ticket feature to numeric and see if it has any classification value:

```{r, cache= TRUE, fig.width= 9}

Ticket <- toupper(training$Ticket)
# Better to remove anything left of the last white space
w <- grep(" ", Ticket)
last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
        max(y[1])
})

Ticket[w] <- substring(Ticket[w],last.space+1)

Ticket <- gsub(" ","", Ticket)
Ticket <- gsub("[A-Z]","",Ticket)
Ticket <- gsub("\\.","",Ticket)
Ticket <- gsub("\\/","", Ticket)
Ticket <- as.numeric(Ticket)
Ticket[is.na(Ticket)] = 0
Ticket <- as.numeric(Ticket)

qplot(x=Ticket, y = Fare, data = training, color = Survived)+theme_bw()+scale_color_manual(values = c("red","navy"))

```

We note that at least two groups of ticket numbers are associated with higher fares, and to some extent they seperate the outcome classes. Therefore, we will engineer this numeric feature and add into all sets, we will remove the original variable.

```{r}
# Feature Engineering function
num.Ticket <- function(x){
Ticket <- toupper(x$Ticket)
# Better to remove anything left of the last white space
w <- grep(" ", Ticket)
last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
        max(y[1])
})
Ticket[w] <- substring(Ticket[w],last.space+1)

Ticket <- gsub(" ","", Ticket)
Ticket <- gsub("[A-Z]","",Ticket)
Ticket <- gsub("\\.","",Ticket)
Ticket <- gsub("\\/","", Ticket)
Ticket <- as.numeric(Ticket)
Ticket <- as.numeric(Ticket)
return(Ticket)
}

training$num.Ticket <- num.Ticket(training)
tune.test.set$num.Ticket <- num.Ticket(tune.test.set)
testing$num.Ticket <- num.Ticket(testing)

sum(is.na(training$num.Ticket ))
sum(is.na(tune.test.set$num.Ticket ))
sum(is.na(testing$num.Ticket))
```

After this conversion, only 2 missing values were introduced to training and tune.testing data sets.

### Name feature: getting 'titles' out of it

When looking into the names of the passengers, we notice that at least we can attempt to extract a "title" feature from the name strings and explore its relationship with the outcome:

```{r}
Pass.Names <- training$Name
first.comma<- sapply(gregexpr(",",Pass.Names), function(y){
        y[1][1]
})
first.dot <- sapply(gregexpr("\\.",Pass.Names), function(y){
        y[1][1]
})
Titles <- substr(Pass.Names,first.comma+2,first.dot-1)
qplot(x = factor(Titles), y = Age ,color = Survived, data = training)
```

This feature can also be useful, therefore we will add into the data sets:

```{r}
# Feature engineering function:
Titles <- function(x){
      Pass.Names <- x$Name
first.comma<- sapply(gregexpr(",",Pass.Names), function(y){
        y[1][1]
})
first.dot <- sapply(gregexpr("\\.",Pass.Names), function(y){
        y[1][1]
})
Titles <- substr(Pass.Names,first.comma+2,first.dot-1)  
return(Titles)
}

training$Titles <- factor(Titles(training))
tune.test.set$Titles <- factor(Titles(tune.test.set))
testing$Titles <- factor(Titles(testing))
```

Next, we need to create dummy variables from each of these titles:

```{r}
factors.training <- which(sapply(training,is.factor))
factors.tune.test.set <- which(sapply(tune.test.set,is.factor))
factors.testing <- which(sapply(testing,is.factor))

dummies.training <- dummyVars(Survived ~ Titles, data = training)
dummies.tune.test.set <- dummyVars(Survived ~ Titles, data = tune.test.set)
dummies.testing <- dummyVars(PassengerId ~ Titles, data = testing)
```

We then add the dummy variables to both training and test data sets, simultaneously removing the existing factor variables:

```{r}
training <- cbind(training[,-factors.training[-1]], predict(dummies.training,newdata = training))
tune.test.set <- cbind(tune.test.set[,-factors.tune.test.set[-1]], predict(dummies.tune.test.set,newdata = tune.test.set))
testing <- cbind(testing[,-factors.testing], predict(dummies.testing,newdata = testing))
```

Not surprisingly ,after doing that we notice that there are differences in the factor levels of Titles feature. At this stage we will only keep features that are present in all sets:

```{r}
training <- training[,names(training) %in% names(tune.test.set)]
tune.test.set <- tune.test.set[,names(tune.test.set) %in% names(training)]
identical(names(training),names(tune.test.set))
PassengerId <- testing$PassengerId
testing <- testing[,names(testing) %in% names(training)]
testing$PassengerId <- PassengerId
```

We should also remove the original Ticket and Name features from all data sets since we already have the engineered versions included:

```{r}
training <- dplyr::select(training,-Ticket,-Name)
tune.test.set <- dplyr::select(tune.test.set ,-Ticket,-Name)
testing <- dplyr::select(testing,-Ticket,-Name)
```

Finally, since we had 2 missing values we generated due to the Ticket feature engineering, we need to get the complete cases in each of the data set:

```{r}
training <- training[complete.cases(training),]
tune.test.set <- tune.test.set[complete.cases(tune.test.set),]
testing <- testing[complete.cases(testing),]
testing <- data.frame(PassengerId = testing$PassengerId, testing[,-27])
```

This completes the feature engineering for all data sets.

___

## Collinearity, Near Zero Variance and Dimension Reduction (Principal Components Analysis)

We generated lots of features! While we had reasons for building them, now it is the chance to drop or transform them if we think that they might hurt us down the road.

One reason why we might want to reduce dimension is the collinearity, where few features are perfectly correlated with each other. Another type are the features with near zero variance.

### Near zero variance features

Let's explore if there are near zero variance features in the training data set:

```{r}
nsv <- nearZeroVar(x = training, saveMetrics = TRUE)
sum(!nsv$nzv)
```

Great we checked, indeed 7 features have near zero variance! We will drop them and keep the remaining 20 in our data sets.

```{r}
training <- training[,!nsv$nzv]
tune.test.set <- tune.test.set[,!nsv$nzv]
testing <- testing[,!nsv$nzv]
```

### Testing for collinearity

Let's have a look at the collinearity in our training set:

```{r}
M <- abs(cor(training[,-1])) # M is an absolute value correlation matrix representing the pairwise #correlations between all variables 
diag(M) <- 0 # We replace the diagonal values with zero (just because these are the correations with  #themselves we are not interested in capturing them).
which(M > 0.8, arr.ind = TRUE) # What are the highest correated variables?
unique(row.names(which(M > 0.8, arr.ind = TRUE)))
cor.variables <- training[,unique(row.names(which(M > 0.8, arr.ind = TRUE)))]
cor.variables$Survived <- training$Survived
```

We notice the expected features correlate with each other. These include Male gender and Title Mr. 

The next question is: **should we replace these features with more statistically orthagonal features by using matrix transformation?**

### Principal Components Analysis (PCA)

A PCA would be handy to visualize whether indeed it is worth reducing the dimension in the space of highly correlated features in our training data set:

```{r}

prePCA <- preProcess(cor.variables[,-6],method = "pca")
PCAcor <- predict(prePCA,cor.variables[,-6])
qplot(PCAcor$PC1,PCAcor$PC2, color = Survived, data = cor.variables)+theme_bw()+scale_color_manual(values = c("red","navy"))
qplot(PCAcor$PC3,PCAcor$PC2, color = Survived, data = cor.variables)+theme_bw()+scale_color_manual(values = c("red","navy"))
```

This was not too helpful, since even the first 3 principal components are unable to explain the variance. At least we have given a try!

We will use the remaining 20 features to train our classifiers.

# Training Classifiers

Due to the classification nature of the problem, we will perform tree-based prediction algorithms and support vector machines to perform predictions. We will perform **10-fold cross validation** in our initial training to prevent over-fitting. We will also set the random number generator seed for reproducibility.

## Simple classification tree with cross validation

```{r,results='markup', message=FALSE,warning=FALSE,}
set.seed(125745)
RPART <- train(Survived ~ ., data = training,method = "rpart", trControl = trainControl(method = "cv", number = 10))
RPART
```

## Random Forest classifier with cross validation

```{r,results='markup', message=FALSE,warning=FALSE,}
set.seed(125745)
RF <- train(Survived ~ ., data = training,method = "rf", trControl = trainControl(method = "cv", number = 10)) 
RF
```

## Boosted tree classifier with cross validation

```{r,results='markup', message=FALSE,warning=FALSE,}
set.seed(125745)
GBM <- train(Survived ~ ., data = training,method = "gbm", trControl = trainControl(method = "cv", number = 10), verbose = FALSE) 
GBM
```

## Support vector machines with a radial kernel

```{r,results='markup', message=FALSE,warning=FALSE,}
set.seed(125745)
SVM <- train(Survived ~ ., data = training,method = "svmRadial", trControl = trainControl(method = "cv", number = 10))
SVM
```

## Summarizing the in-sample accuracy metrics from the individual classifiers

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
# Using resamples function from the caret package to summarize the data
modelsummary <- resamples(list(RPART=RPART,RF=RF,GBM=GBM,SVM=SVM))
# In-sample accuracy values for each model
summary(modelsummary)$statistics$Accuracy
```

If we consider the Median accuracy as our benchmark, the Random Forest classifier seems to have the best accuracy amongst the classifiers we trained. Our out of the sample accuracy by using the tune.test set will be much lower though.

## 'Variable importance' plots for each classifier

```{r}
dotPlot(varImp(RPART))
dotPlot(varImp(RF))
dotPlot(varImp(SVM))
dotPlot(varImp(GBM))
```

It is great to see that some of the features we engineered are amongst the most important features for training the classifiers!

## Tuning and stacking the classifiers for improved accuracy

### Random Forest classifier with bootstrap and 632 correction

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RF.tune <- train(Survived ~ ., data = training,method = "rf", trControl = trainControl(method = "boot632", number = 10) )
median(RF.tune$results$Accuracy)
```

Great! Using this approach increased in sample accuracy to almost 88%!

### Stacking classifiers

Let's try to build a **'classifier of classifiers'** by using the predictions from the individual predictors. Ideally we should perform this in a seperate test set, however the data set we have is relatively small so we will have to reuse the predictions from the test data set.

```{r}
in.sample.predictions <- data.frame(RF.tune = predict(RF.tune,training), RF = predict(RF, training), GBM = predict(GBM,training), RPART = predict(RPART, training), SVM = predict(SVM, training))

# Add the outcome
in.sample.predictions$Survived <- training$Survived
```

We will stack these classifiers and train two more classifiers by using the Random Forest and Gradient boosted tree classifiers

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RF.stack <- train(Survived ~ ., data = in.sample.predictions,method = "rf", trControl = trainControl(method = "cv", number = 10)) 
RF.stack
```

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
GBM.stack <- train(Survived ~ ., data = in.sample.predictions,method = "gbm", trControl = trainControl(method = "cv", number = 10), verbose = FALSE) 
GBM.stack
```

That is great! Stacking the classifiers really improved the accuracy and we have now over 99% in-sample accuracy. There might be a little bit over-fitting since we reused the training set multiple times, but we will asses our out-of the sample accuracy by using the tune.test.set.

# Unbiased evaluation of the out-of-the box accuracy using the tune test set

Before making our prediction with the actual test data set, it is a good idea to get an estimate of our out-of the box accuracy by using the tune.test.set we randomly partitioned initially. Since we haven't touched this data set for training our classifiers, this would be a good first hand estimate of the out-of-the box accuracy.

Since we will use the stacked classifiers we constructed, we will have to follow the same procedure in the tune.test data set to perform predictions using our trained classifiers

```{r}
pred.tune.test.RF <- predict(RF, tune.test.set)
pred.tune.test.RF.tune <- predict(RF.tune, tune.test.set)
pred.tune.test.GBM <- predict(GBM, tune.test.set)
pred.tune.test.RPART <- predict(RPART, tune.test.set)
pred.tune.test.SVM <- predict(SVM, tune.test.set)

tune.test.predictions <- data.frame(RF.tune = pred.tune.test.RF.tune, RF = pred.tune.test.RF, GBM = pred.tune.test.GBM, RPART = pred.tune.test.RPART, SVM = pred.tune.test.SVM)

# Add the outcome from tune.test set
tune.test.predictions$Survived <-tune.test.set$Survived

```

Next, make the predictions: 

```{r}
pred.tune.test.RF.stack <- predict(RF.stack,tune.test.predictions)
pred.tune.test.GBM.stack <- predict(GBM.stack,tune.test.predictions)

confusionMatrix(pred.tune.test.RF.stack, tune.test.set$Survived)
confusionMatrix(pred.tune.test.GBM.stack, tune.test.set$Survived)
```

# Conclusions

The out-of-the sample accuracy of the stacked classifiers is ~ 83%. This is probably because we did not use a third data set in between when stacking our classifiers. This likely caused over-fitting of the stacked classifiers and overestimation of the accuracy.

Nevertheless, it was worth trying to build the stacked classifiers, as they still have improved accuracy relative to stand alone classifiers.

# Final prediction and preparation of the submission

Our final step is to make predictions using the unlabeled testing data set and prepare a submission file that matches our prediction with the Passenger IDs. We will use the stacked Random Forest classifier as it gave us the highest out-of-the sample accuracy:

```{r}
pred.test.RF <- predict(RF, testing[,-1])
pred.test.RF.tune <- predict(RF.tune, testing[,-1])
pred.test.GBM <- predict(GBM, testing[,-1])
pred.test.RPART <- predict(RPART, testing[,-1])
pred.test.SVM <- predict(SVM, testing[,-1])

test.predictions <- data.frame(RF.tune = pred.test.RF.tune, RF = pred.test.RF , GBM = pred.test.GBM, RPART = pred.test.RPART, SVM = pred.test.SVM)

pred.test.RF.stack <- as.numeric(as.character(predict(RF.stack,test.predictions)))

```

# Revisiting the classification problem

In this new section, I will revisit the data wth a fresh look and see if I can try alternative approaches to improve the classification accuracy.

## Using Principal Components to train classifiers: beating the curse of dimensionality

Perhaps it was a little wasteful to seperate a validation set from the small training set. Since we already perform cross-validation by training our model, in the rest of my attempts I decided to use the entire training set to train the classifiers.

### Feature selection and engineering

```{r, fig.width= 6, fig.height= 8}
library(dplyr);library(ggplot2); library(caret)

training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")


training <- dplyr::select(training, -PassengerId)
training$Survived <- factor(training$Survived)

# Fare is important to keep, Age perhaps not
qplot(x = Age, y = Fare, data = training, color = Survived)

# Also keep Pclass (good at classifiying victims)
qplot(x = Pclass, y = Fare, data = training, color = Survived, alpha = I(0.2))


# Leave Pclass and Parch
F.size = training$Parch + training$SibSp
qplot(x = F.size , y = Fare, data = training, color = Survived, alpha = I(0.4))

# Definitely keep Gender
qplot(x = Sex , y = Fare, data = training, color = Survived, alpha = I(0.4))


# remove other features:

training <- dplyr::select(training, -Name, -Age, -Ticket,-Cabin, - Embarked)
final_testing <- dplyr::select(final_testing, -Name, -Age, -Ticket,-Cabin, - Embarked)

summary(training)
summary(final_testing)

# There is one missing value in the Fare feature in the testing set, thus we need to find a way to impute by using the 
# training set

lmFit1 <- lm(Fare ~ factor(Pclass)*factor(Sex), data = training)

summary(lmFit1)

par(mfrow= c(2,2))
plot(lmFit1)[1:4]

# lmFit1 Fair model to impute Fare in the testing set

sum(is.na(training$Fare)) # No missing values in the training set
sum(is.na(final_testing$Fare)) # One missing value in the test set

final_testing$Fare[is.na(final_testing$Fare)] <- predict(lmFit1, newdata = final_testing[is.na(final_testing$Fare),])

sum(is.na(final_testing$Fare)) # Imputed

# Final check confirms that there is no remaining missing data in either of the data sets
apply(is.na(training),2,sum);apply(is.na(final_testing),2,sum)


# Convert Sex to dummy variable

training$Female <- ifelse(training$Sex == "female", 1,0)
final_testing$Female <- ifelse(final_testing$Sex == "female", 1,0)

training <- dplyr::select(training, - Sex)
final_testing <- dplyr::select(final_testing, -Sex)

```

This provided us a fairly small set of features that are likely to be important for classification. 

### Dimension reduction with the training set

Let's now see whether Principal Components can aid classification:

```{r, fig.width=6, fig.height=6}
prePCA <- preProcess(training[,-1],method = "pca")
PCAtraining <- predict(prePCA,newdata = training)




qplot(x = PC1, y = PC2, data = PCAtraining, color = Survived, alpha = I(0.3))+theme_bw()+
        scale_color_manual(values = c("red","navy"))

qplot(x = PC2, y = PC3, data = PCAtraining, color = Survived, alpha = I(0.3))+theme_bw()+
        scale_color_manual(values = c("red","navy"))

qplot(x = PC1, y = PC3, data = PCAtraining, color = Survived, alpha = I(0.3))+theme_bw()+
        scale_color_manual(values = c("red","navy"))

```

PCA predictors seperate classes pretty well!

### Training classifiers using Principal Components as predictors

After explaining most of the variability in the existing predictors, what happens if we train our classifiers with these principal components ?

```{r}

set.seed(1234)
PCAknn <- train(Survived ~ ., data = PCAtraining,method = "knn",
                trControl= trainControl(method = "boot632"))



# So far gives the best accuracy: (bench mark: 0.83)##############
set.seed(1234)
PCA.rf <- train(Survived ~ ., data = PCAtraining,method = "rf",
                   trControl= trainControl(method = "boot632"))
##################################################################
# Not as good as the PCA.rf
set.seed(1234)
PCA.gbm <- train(Survived ~ ., data = PCAtraining,method = "gbm",
                trControl= trainControl(method = "boot632", number = 200),verbose =F)

# Not as good as the PCA.rf
set.seed(1234)
PCA.lda <- train(Survived ~ ., data = PCAtraining,method = "lda",
                 trControl= trainControl(method = "boot632"), verbose =F)

# Not as good as the PCA.rf
set.seed(1234)
PCA.svm <- train(Survived ~ ., data = PCAtraining,method = "svmRadial",
                 trControl= trainControl(method = "boot632"), verbose =F)

# Let's make a prediction by using final testing set:

PCAtesting <- predict(prePCA,newdata = final_testing)

predictions <- NULL

models <- list(PCA.lda,
               PCA.gbm,
               PCA.rf,
               PCA.amdai,
               PCAknn,
               PCA.svm)

predictions <- sapply(models, function(x){
        temp <- as.numeric(as.character(predict(x,newdata = PCAtesting[,-1])))
})

```

```{r, eval=FALSE}
prediction.table <- NULL
for(i in seq_along(predictions)){
        prediction.table <- data.frame(PassengerId = final_testing$PassengerId, 
                                          Survived = predictions[,i])   
        write.csv(prediction.table,paste0("PCA_predictions",i,".csv"), row.names = F)
}

```


- PCA.svm performed better than any other classifier I trained so far! The test accuracy was : **0.77990**

- Surprisingly, PCA.lda was the second best one!

**Take home message:** reduce the dimension as much as possible, try to explain more variability in the data with a smaller set of features, and keep trying different models despite their training accuracy, they might perform better in the test set.

This brings us a slight rise in the leaderborad, but certainly we have more way to go.

## Using Principal Components to train classifiers: what if we added those few more features while training the classifiers?

Let's go back to perform: 

- a few more feature engineering steps to add some features I was able to extract previously
- repeat PCA 
- fit PCA.svm again
- Test whether adding a few more features before the PCA can improve the accuracy (hoping that we can capture more variance in the direction of the response, in the form of principal components) 

### Feature selection and engineering

```{r, fig.width= 6, fig.height= 6}
library(dplyr);library(ggplot2); library(caret)

training <- read.csv("train.csv")
final_testing <- read.csv("test.csv")


training <- dplyr::select(training, -PassengerId)
training$Survived <- factor(training$Survived)

# remove some features:

training <- dplyr::select(training,-Cabin, - Embarked, -Age)
final_testing <- dplyr::select(final_testing,-Cabin, - Embarked, -Age)

summary(training)
summary(final_testing)

# There is one missing value in the Fare feature in the testing set, thus we need to find a way to impute by using the 
# training set

lmFit1 <- lm(Fare ~ factor(Pclass)*factor(Sex), data = training)

summary(lmFit1)

par(mfrow= c(2,2))
plot(lmFit1)[1:4]

# lmFit1 Fair model to impute Fare in the testing set

sum(is.na(training$Fare)) # No missing values in the training set
sum(is.na(final_testing$Fare)) # One missing value in the test set

final_testing$Fare[is.na(final_testing$Fare)] <- predict(lmFit1, newdata = final_testing[is.na(final_testing$Fare),])

sum(is.na(final_testing$Fare)) # Imputed

# Final check confirms that there is no remaining missing data in either of the data sets
apply(is.na(training),2,sum);apply(is.na(final_testing),2,sum)

# Convert Sex to dummy variable
training$Female <- ifelse(training$Sex == "female", 1,0)
final_testing$Female <- ifelse(final_testing$Sex == "female", 1,0)

training <- dplyr::select(training, - Sex)
final_testing <- dplyr::select(final_testing, -Sex)


# Ticket
Ticket <- toupper(training$Ticket)
# Better to remove anything left of the last white space
w <- grep(" ", Ticket)
last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
        max(y[1])
})

Ticket[w] <- substring(Ticket[w],last.space+1)

Ticket <- gsub(" ","", Ticket)
Ticket <- gsub("[A-Z]","",Ticket)
Ticket <- gsub("\\.","",Ticket)
Ticket <- gsub("\\/","", Ticket)
Ticket <- as.numeric(Ticket)
Ticket[is.na(Ticket)] = 0
Ticket <- as.numeric(Ticket)


plot(x=log10(Ticket), y = training$Fare, col = ifelse(training$Survived == 0, "red","navy"))
plot(x=log10(Ticket), y = training$SibSp, col = ifelse(training$Survived == 0, "red","navy"))
plot(x=log10(Ticket), y = training$Parch, col = ifelse(training$Survived == 0, "red","navy"))

# Remove original Ticket and add new Ticket feature

# Feature Engineering function
num.Ticket <- function(x){
        Ticket <- toupper(x$Ticket)
        # Better to remove anything left of the last white space
        w <- grep(" ", Ticket)
        last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
                max(y[1])
        })
        Ticket[w] <- substring(Ticket[w],last.space+1)
        
        Ticket <- gsub(" ","", Ticket)
        Ticket <- gsub("[A-Z]","",Ticket)
        Ticket <- gsub("\\.","",Ticket)
        Ticket <- gsub("\\/","", Ticket)
        Ticket <- as.numeric(Ticket)
        Ticket <- as.numeric(Ticket)
        return(log10(Ticket))
}

training$num.Ticket <- num.Ticket(training)
final_testing$num.Ticket <- num.Ticket(final_testing)


sum(is.na(training$num.Ticket )) # 4 Missing values introduced
sum(is.na(final_testing$num.Ticket)) # No missing values

# remove the Ticket feature

training <- dplyr::select(training,-Ticket)
final_testing <- dplyr::select(final_testing,-Ticket)

# Title feature from Name:

# Feature engineering function:
Titles.func <- function(x){
        Pass.Names <- x$Name
        
        first.comma<- sapply(gregexpr(",",Pass.Names), function(y){
                y[1][1]
        })
        first.dot <- sapply(gregexpr("\\.",Pass.Names), function(y){
                y[1][1]
        })
        
        Titles <- substr(Pass.Names,first.comma+2,first.dot-1)  
        return(Titles)
        }

Titles <- Titles.func(training)

qplot(x = training$Fare[training$Female == 0], y = factor(Titles[training$Female == 0]), data = training[training$Female == 0,], color = Survived, alpha = I(0.2))+
        theme_bw()

# We notice that "Master" title explains quite a few of the survived males. This would be a useful feature to add into the data sets

training$Titles.Master <- ifelse(Titles == "Master",1,0)
final_testing$Titles.Master <- ifelse(Titles.func(final_testing) == "Master",1,0)

# Remove the original Name feature:

training <- dplyr::select(training,-Name)
final_testing <- dplyr::select(final_testing,-Name)

apply(is.na(training),2,sum)
apply(is.na(final_testing),2,sum)

training <- training[complete.cases(training),]
```

Now we added 2 more features compared to our previous attempt. Let's build our classifiers to see if that made any difference.

### Dimension reduction with the training set

```{r}

prePCA <- preProcess(training[,-1],method = "pca")
PCAtraining <- predict(prePCA,newdata = training)

qplot(x = PC1, y = PC2, data = PCAtraining, color = Survived, alpha = I(0.3))+theme_bw()+
        scale_color_manual(values = c("red","navy"))
```

The decision boundary is still not an easy one for any classifier. But we can't tell if we have improved the accuracy until we make some predictions!

```{r}
set.seed(1234)
PCA.svm <- train(Survived ~ ., data = PCAtraining,method = "svmRadial",
                 trControl= trainControl(method = "boot632"), verbose =F)

# Let's perform a prediction:

PCAtesting <- predict(prePCA,newdata = final_testing)


```

```{r,eval=FALSE}
prediction.table <- data.frame(PassengerId = final_testing$PassengerId, 
                               Survived = predict(PCA.svm,PCAtesting))   
write.csv(prediction.table,paste0("PCA_predictions_3",".csv"), row.names = F)

```

Great! These two new features alone have increased the testing accuracy from **0.77 to 0.79426!**

Take home message: every small twist makes a huge difference!

# Using Shrinkage methods (Regularization)

It would be also interesting to perform a linear model selection and see how well it might perform with the new training set we obtained.

## Fitting a lasso logistic regression model to data

```{r}
library(glmnet)

# We convert Pclass, SibSp and Parch into factor variables

training$Pclass <- factor(training$Pclass)
training$SibSp <- factor(training$SibSp)
training$Parch <- factor(training$Parch)

final_testing$Pclass <- factor(final_testing$Pclass)
final_testing$SibSp <- factor(final_testing$SibSp)
final_testing$Parch <- factor(final_testing$Parch)

# We create a matrix of predictors
x.training = model.matrix(Survived ~ . -1, data = training)
x.testing = model.matrix( ~ . -1, data = final_testing)


y = training$Survived

# Fit lasso regression to training data
fit.lasso <- glmnet(x.training,y, family = "binomial")
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)

```

Lasso is a nice way of reducing the features while still trying to explain the variance and therefore trying to win the bias-variance tradeoff. In this case we notice that only 4 features are able to explain 30% of the deviance, while adding all features is only able to explain 40% of it. It is more feasible to use this restricted model with the few features and their shrunken coefficients.

## Choosing the optimal lambda by using cross-validation

It is best to choose the optimal lambda using cross-validation, with the aim of minimizing the missclassification error:

```{r}

```


```