---
title: "This is just the scratchpad for the analysis not the final report"
author: "Ozan Aygun"
date: "5/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 5)
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Competition-Titanic")
```

Load the data sets:

```{r}
training <- read.csv("train.csv", stringsAsFactors = FALSE, na.strings = "")
testing <- read.csv("test.csv", stringsAsFactors = FALSE,na.strings = "")
```

Partition the training set into:

- training: train different models
- tune.test.set: initial out of the box performance

```{r,results='markup'}
library(caret)
set.seed(1234)
InTrain <- createDataPartition(y=training$Survived,p = 0.7,list = FALSE)

tune.test.set <- training[-InTrain,]
training <- training[InTrain,]

```



Summarize the training data set:
```{r}
summary(training)
table(training$Sex)
table(training$Cabin)
table(training$Embarked)
```


# Basic preprocessing and EDA (all processing performed in the building set and exactly applied to tune.testing,validation, and final_testing sets)


```{r}
#Exploration by pairs plot:
pairs(Survived ~ Age+ SibSp+Parch+Fare,pch =19, cex = 0.4,data=training,
      col= ifelse(training$Survived == 1, "navy","red"))

# Clearly less people survived from the 3rd class
table(training$Survived,training$Pclass)

table(training$Survived, training$Embarked)

# Clearly more Females survived
table(training$Survived,training$Sex)



# Consistent with the Class ~ Survival relationship
boxplot(log(Fare) ~ Survived, data = training)


library(ggplot2)
ggplot(aes(x = SibSp, fill = factor(Survived)), data = training)+
        geom_bar(stat = "count")+
        theme_bw()

ggplot(aes(x = Parch, fill = factor(Survived)), data = training)+
        geom_bar(stat = "count")+
        theme_bw()

boxplot(SibSp ~ Survived, data = training)
boxplot(Parch ~ Survived, data = training)




```

## Generate factor (categorical variables)
```{r}
training <- transform(training,Survived = factor(Survived), Pclass = factor(Pclass),
          Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))
tune.test.set <- transform(tune.test.set,Survived = factor(Survived), Pclass = factor(Pclass),Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))

testing <- transform(testing, Pclass = factor(Pclass),
          Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))

```

## Remove the passenger ID column from the training sets.

```{r}
library(dplyr)
training <- dplyr::select(training,-PassengerId)
tune.test.set <- dplyr::select(tune.test.set,-PassengerId)
```

# Investigate the missing values

```{r}
training.na <- as.data.frame(is.na(training));names(training.na) <- names(training)
apply(training.na,2,sum)
```

Missing values appear in 3 variables; Age, Cabin and Embarked.

### Cabin feature: more than 75% is missing

From the available observations, we can infer that the first letter actually presents the cabin section.

Extract the first letter from the available ones and test if they have any predictive value:

```{r}
library(ggplot2); library(dplyr)
Cabin.letter <-substr(training$Cabin[!training.na$Cabin],1,1)
Cabin.survival <- training$Survived[!training.na$Cabin]
Cabin.Pclass <- training$Pclass[!training.na$Cabin]
qplot(x = factor(Cabin.letter), fill = Cabin.survival)+scale_fill_manual(values = c("red","navy"))+theme_bw()
```
We notice that this feature has some predictive value. 

We check if the cabin relates with the Pclass:
```{r}
qplot(x = factor(Cabin.letter), fill = Cabin.Pclass)+scale_fill_manual(values = c("red","navy","green"))+theme_bw()
```
We indeed notice that Cabin letters A,B,C are absolutely first class and therefore can be inferred from the Pclass variable. D and E are also more likely to be in 1st class. Gs are all coming from the 3rd Class. 

We can also check if fare relates with the cabin letter:
```{r}
Cabin.Fare <- training$Fare[!training.na$Cabin]
qplot(x = factor(Cabin.letter), y=Cabin.Fare, color = Cabin.survival)+theme_bw()
```
The relationship between Fare and Cabin Letter is not so dramatic to allow some imputation. 

The problem with imputing Cabin feature from Pclass is that we don't know whether the missing variables are MCAR (Missing Completely at Random) or whether there is a relationship between the reason they are missing and the outcome (Survival).

Therefore, it would be more sensible to drop this variable and don't use in building our classifiers.

####Remove Cabin feature from all sets:

```{r}
training <- dplyr::select(training, - Cabin)
tune.test.set <- dplyr::select(tune.test.set, -Cabin)
testing <- dplyr::select(testing, -Cabin)
```

## Age feature: ~ 20% missing data

One intuitive imputation potential could be comparing the Age with SibSp feature:

```{r}
qplot(x = SibSp, y = Age, color = Survived, data = training)+
       theme_bw()
  
```
We notice that as the SibSp increases, the age group decreases. 
```{r}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ SibSp)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
```
Most of the Age distribution is having either no Spouses or siblings or only 1.  For the 1 siblings/spouses group the mean age is higher. In both cases Age is appromated by normal distribution. The problem with SibSp is that some of the factor levels are only present in the observations where Age is missing, making this predictor unbalanced across the missing and complete cases.

It would be also interesting to explore gender differences when considering Age:
```{r}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ Sex)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
```
How about the Age distribution in different passenger classes? (Pclass):

```{r}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ Pclass)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
training %>% group_by(Pclass,Survived) %>% summarise(mean(Age,na.rm=T))
```
This is quite interesting! In all passenger classes, the mean age of the survived passengers is lower than those passed away. We also notice that the mean age decreases and the Class number increases, i.e: older passengers are in the better classes on average. Therefore, Pclass would also be include in the imputation model.

It would be therefore sensible to impute Age by random gaussian imputation using the mean and standard deviation of individual factor levels of these predictors. We will include Sex in this model to account for gender-specific differences in Age, as well as Pclass to account for the interesting seperation of Age by Passenger Class we noted above. In order to do this more rigorously, we can first fit a linear model with the existing Age , Sex  and Pclass  data:

##### Age ~  Sex + Pclass + e (random gaussian error)

```{r}
lmAge = lm(Age ~ Sex + Pclass, data = training, na.action = "na.omit")
summary(lmAge)
par(mfrow = c(2,2))
plot(lmAge)[1]
plot(lmAge)[2]
plot(lmAge)[3]
plot(lmAge)[4]
```
This model is just OK, but we don't need the perfect model for this type of imputation. Nice to see that both variance and normality assumptions of the model holds and all levels of the covariates have significant impact on the mean outcome in the presence of each other. This would give us a good estimation for the missing values of Age.

Using the model to impute missing values of Age:

```{r}
training$Age[is.na(training$Age)] = predict(lmAge, newdata = training[is.na(training$Age),])
tune.test.set$Age[is.na(tune.test.set$Age)] = predict(lmAge, newdata = tune.test.set[is.na(tune.test.set$Age),])
testing$Age[is.na(testing$Age)] = predict(lmAge, newdata = testing[is.na(testing$Age),])

```
Note that we use the same model object we derived from the training data set to impute the missing values of Age for all data sets. This will prevent us from overfitting to the test data set.

Re-investigate the missing values:
```{r}
training.na <- as.data.frame(is.na(training));names(training.na) <- names(training)
apply(training.na,2,sum)

tune.testing.na <- as.data.frame(is.na(tune.test.set));names(tune.testing.na) <- names(tune.test.set)
apply(tune.testing.na ,2,sum)

testing.na <- as.data.frame(is.na(testing));names(testing.na) <- names(testing)
apply(testing.na ,2,sum)

```
It appears that only one case is left missing in each of the data sets. We will consider this as random missingness and remove in each data set:

```{r}
training <- training[complete.cases(training),]
tune.test.set <- tune.test.set[complete.cases(tune.test.set),]
testing <- testing[complete.cases(testing),]
```

Now we have completed processing the missing values.


#Feature Engineering:

## Generate Dummy Variables with Factor variables

Better to perform this step at the very end for all categorical variables.

```{r}
library(caret)

which(sapply(training[,-1],is.factor))
# There are 5 categorical predictors in out data set

factors.training <- which(sapply(training,is.factor))
factors.tune.test.set <- which(sapply(tune.test.set,is.factor))
factors.testing <- which(sapply(testing,is.factor))


dummies.training <- dummyVars(Survived ~ Pclass + Sex + SibSp + Parch + Embarked, data = training)

dummies.tune.test.set <- dummyVars(Survived ~ Pclass + Sex + SibSp + Parch + Embarked, data = tune.test.set)

dummies.testing <- dummyVars(PassengerId ~ Pclass + Sex + SibSp + Parch + Embarked, data = testing)


# Add the dummy variables to both training and test data sets, simultaneously removing the existing factor variables:
training <- cbind(training[,-factors.training[-1]], predict(dummies.training,newdata = training))
tune.test.set <- cbind(tune.test.set[,-factors.tune.test.set[-1]], predict(dummies.tune.test.set,newdata = tune.test.set))
testing <- cbind(testing[,-factors.testing], predict(dummies.testing,newdata = testing))
```

After doing that we notice that there are differences in the factor levels of certain variables are not present in the training set and test sets. These are present in Parch and SibSp features. The lower levels of these variables are conserved in all sets and they represent most of the data. At this stage we will only keep features that are present in all sets:

```{r}
training <- training[,names(training) %in% names(tune.test.set)]
tune.test.set <- tune.test.set[,names(tune.test.set) %in% names(training)]

identical(names(training),names(tune.test.set))

PassengerId <- testing$PassengerId

testing <- testing[,names(testing) %in% names(training)]
testing$PassengerId <- PassengerId

```

### Feature engineering with Name and Ticket variables

### Ticket feature:

It would be interesting to just convert the ticket feature to numeric and see if it has any seperation value:
```{r}

Ticket <- toupper(training$Ticket)
# Better to remove anything left of the last white space
w <- grep(" ", Ticket)
last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
        max(y[1])
})

Ticket[w] <- substring(Ticket[w],last.space+1)

Ticket <- gsub(" ","", Ticket)
Ticket <- gsub("[A-Z]","",Ticket)
Ticket <- gsub("\\.","",Ticket)
Ticket <- gsub("\\/","", Ticket)
Ticket <- as.numeric(Ticket)
Ticket[is.na(Ticket)] = 0
Ticket <- as.numeric(Ticket)


qplot(x=Ticket, y = Fare, data = training, color = Survived)+theme_bw()

```
We note that at least two groups of ticket numbers are associated with higher fares, and to some extent they seperate the outcome classes. Therefore, we will engineer this numeric feature and add into all sets, we will remove the original variable.

```{r}
# Feature Engineering function

num.Ticket <- function(x){
Ticket <- toupper(x$Ticket)
# Better to remove anything left of the last white space
w <- grep(" ", Ticket)
last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
        max(y[1])
})
Ticket[w] <- substring(Ticket[w],last.space+1)

Ticket <- gsub(" ","", Ticket)
Ticket <- gsub("[A-Z]","",Ticket)
Ticket <- gsub("\\.","",Ticket)
Ticket <- gsub("\\/","", Ticket)
Ticket <- as.numeric(Ticket)
Ticket <- as.numeric(Ticket)
return(Ticket)
}

training$num.Ticket <- num.Ticket(training)
tune.test.set$num.Ticket <- num.Ticket(tune.test.set)
testing$num.Ticket <- num.Ticket(testing)


sum(is.na(training$num.Ticket ))
sum(is.na(tune.test.set$num.Ticket ))
sum(is.na(testing$num.Ticket))
```
After this conversion, only 2 missing values were introduced to training and tune.testing data sets.

### Name feature:

When looking into the names of the passengers, we notice that at least we can attempt to extract a "title" feature from the name strings and explore its relationship with the outcome:

```{r}
Pass.Names <- training$Name

first.comma<- sapply(gregexpr(",",Pass.Names), function(y){
        y[1][1]
})
first.dot <- sapply(gregexpr("\\.",Pass.Names), function(y){
        y[1][1]
})

Titles <- substr(Pass.Names,first.comma+2,first.dot-1)

qplot(x = factor(Titles), y = Age ,color = Survived, data = training)
```

This feature can also be useful, therefore we will add into the data sets:

```{r}
# Feature engineering function:
Titles <- function(x){
      Pass.Names <- x$Name

first.comma<- sapply(gregexpr(",",Pass.Names), function(y){
        y[1][1]
})
first.dot <- sapply(gregexpr("\\.",Pass.Names), function(y){
        y[1][1]
})

Titles <- substr(Pass.Names,first.comma+2,first.dot-1)  
return(Titles)
        
}

training$Titles <- factor(Titles(training))
tune.test.set$Titles <- factor(Titles(tune.test.set))
testing$Titles <- factor(Titles(testing))

```
Next, we need to create dummy variables from each of these titles:

```{r}
factors.training <- which(sapply(training,is.factor))
factors.tune.test.set <- which(sapply(tune.test.set,is.factor))
factors.testing <- which(sapply(testing,is.factor))


dummies.training <- dummyVars(Survived ~ Titles, data = training)

dummies.tune.test.set <- dummyVars(Survived ~ Titles, data = tune.test.set)

dummies.testing <- dummyVars(PassengerId ~ Titles, data = testing)
```

# Add the dummy variables to both training and test data sets, simultaneously removing the existing factor variables:
```{r}
training <- cbind(training[,-factors.training[-1]], predict(dummies.training,newdata = training))
tune.test.set <- cbind(tune.test.set[,-factors.tune.test.set[-1]], predict(dummies.tune.test.set,newdata = tune.test.set))
testing <- cbind(testing[,-factors.testing], predict(dummies.testing,newdata = testing))

```

Not surprisingly ,after doing that we notice that there are differences in the factor levels of Titles feature. At this stage we will only keep features that are present in all sets:

```{r}
training <- training[,names(training) %in% names(tune.test.set)]
tune.test.set <- tune.test.set[,names(tune.test.set) %in% names(training)]

identical(names(training),names(tune.test.set))

PassengerId <- testing$PassengerId

testing <- testing[,names(testing) %in% names(training)]
testing$PassengerId <- PassengerId

```

We should also remove the original Ticket and Name features from all data sets since we already have the engineered versions included:

```{r}
training <- dplyr::select(training,-Ticket,-Name)
tune.test.set <- dplyr::select(tune.test.set ,-Ticket,-Name)
testing <- dplyr::select(testing,-Ticket,-Name)
```

Finally, since we had 2 missing values we generated due to the Ticket feature engineering, we need to get the complete cases in each of the data set:

```{r}
training <- training[complete.cases(training),]
tune.test.set <- tune.test.set[complete.cases(tune.test.set),]
testing <- testing[complete.cases(testing),]

testing <- data.frame(PassengerId = testing$PassengerId, testing[,-27])
```

This completes the feature engineering for all data sets.

## Collinearity, Near Zero Variance and Dimension Reduction

# Remove near-zero variance features:

Once completed the feature engineering, let's explore if there are near zero variance features in the training data set:

```{r}
nsv <- nearZeroVar(x = training, saveMetrics = TRUE)
sum(!nsv$nzv)
```
20 out of the 27 variables have non-zero variance and will be kept in the data sets.
```{r}
training <- training[,!nsv$nzv]
tune.test.set <- tune.test.set[,!nsv$nzv]
testing <- testing[,!nsv$nzv]
```

## Test for high correlation:

```{r}
M <- abs(cor(training[,-1])) # M is an absolute value correlation matrix representing the pairwise #correlations between all variables 
diag(M) <- 0 # We replace the diagonal values with zero (just because these are the correations with  #themselves we are not interested in capturing them).
which(M > 0.8, arr.ind = TRUE) # What are the highest correated variables?
unique(row.names(which(M > 0.8, arr.ind = TRUE)))

cor.variables <- training[,unique(row.names(which(M > 0.8, arr.ind = TRUE)))]
cor.variables$Survived <- training$Survived
```

## Principal components analysis

Next we will perform PCA to see if dimension reduction might help to reduce the highly correlated predictors in the data set:

```{r}
prePCA <- preProcess(cor.variables[,-6],method = "pca")
PCAcor <- predict(prePCA,cor.variables[,-6])
qplot(PCAcor$PC1,PCAcor$PC2, color = Survived, data = cor.variables)
qplot(PCAcor$PC3,PCAcor$PC2, color = Survived, data = cor.variables)
```

Since these are all binary variables, PCA did not furher help to reduce the dimensions.

We will use the remaining 20 features to train our classifiers.

## Training Classifiers
