---
title: "This is just the scratchpad for the analysis not the final report"
author: "Ozan Aygun"
date: "5/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 5)
setwd("~/Desktop/2016/Data_science/Kaggle/Kaggle-Competition-Titanic")
```

Load the data sets:

```{r}
training <- read.csv("train.csv", stringsAsFactors = FALSE, na.strings = "")
testing <- read.csv("test.csv", stringsAsFactors = FALSE,na.strings = "")
```

Partition the training set into:

- training: train different models
- tune.test.set: initial out of the box performance

```{r,results='markup'}
library(caret)
set.seed(1234)
InTrain <- createDataPartition(y=training$Survived,p = 0.7,list = FALSE)

tune.test.set <- training[-InTrain,]
training <- training[InTrain,]

```



Summarize the training data set:
```{r}
summary(training)
table(training$Sex)
table(training$Cabin)
table(training$Embarked)
```


# Basic preprocessing and EDA (all processing performed in the building set and exactly applied to tune.testing,validation, and final_testing sets)


```{r}
#Exploration by pairs plot:
pairs(Survived ~ Age+ SibSp+Parch+Fare,pch =19, cex = 0.4,data=training,
      col= ifelse(training$Survived == 1, "navy","red"))

# Clearly less people survived from the 3rd class
table(training$Survived,training$Pclass)

table(training$Survived, training$Embarked)

# Clearly more Females survived
table(training$Survived,training$Sex)



# Consistent with the Class ~ Survival relationship
boxplot(log(Fare) ~ Survived, data = training)


library(ggplot2)
ggplot(aes(x = SibSp, fill = factor(Survived)), data = training)+
        geom_bar(stat = "count")+
        theme_bw()

ggplot(aes(x = Parch, fill = factor(Survived)), data = training)+
        geom_bar(stat = "count")+
        theme_bw()

boxplot(SibSp ~ Survived, data = training)
boxplot(Parch ~ Survived, data = training)




```

## Generate factor (categorical variables)
```{r}
training <- transform(training,Survived = factor(Survived), Pclass = factor(Pclass),
          Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))
tune.test.set <- transform(tune.test.set,Survived = factor(Survived), Pclass = factor(Pclass),Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))

testing <- transform(testing, Pclass = factor(Pclass),
          Sex = factor(Sex),SibSp = factor(SibSp), Parch = factor(Parch), Embarked = factor(Embarked))

```

## Remove the passenger ID column from the training sets.

```{r}
library(dplyr)
training <- dplyr::select(training,-PassengerId)
tune.test.set <- dplyr::select(tune.test.set,-PassengerId)
```

# Investigate the missing values

```{r}
training.na <- as.data.frame(is.na(training));names(training.na) <- names(training)
apply(training.na,2,sum)
```

Missing values appear in 3 variables; Age, Cabin and Embarked.

### Cabin feature: more than 75% is missing

From the available observations, we can infer that the first letter actually presents the cabin section.

Extract the first letter from the available ones and test if they have any predictive value:

```{r}
library(ggplot2); library(dplyr)
Cabin.letter <-substr(training$Cabin[!training.na$Cabin],1,1)
Cabin.survival <- training$Survived[!training.na$Cabin]
Cabin.Pclass <- training$Pclass[!training.na$Cabin]
qplot(x = factor(Cabin.letter), fill = Cabin.survival)+scale_fill_manual(values = c("red","navy"))+theme_bw()
```
We notice that this feature has some predictive value. 

We check if the cabin relates with the Pclass:
```{r}
qplot(x = factor(Cabin.letter), fill = Cabin.Pclass)+scale_fill_manual(values = c("red","navy","green"))+theme_bw()
```
We indeed notice that Cabin letters A,B,C are absolutely first class and therefore can be inferred from the Pclass variable. D and E are also more likely to be in 1st class. Gs are all coming from the 3rd Class. 

We can also check if fare relates with the cabin letter:
```{r}
Cabin.Fare <- training$Fare[!training.na$Cabin]
qplot(x = factor(Cabin.letter), y=Cabin.Fare, color = Cabin.survival)+theme_bw()
```
The relationship between Fare and Cabin Letter is not so dramatic to allow some imputation. 

The problem with imputing Cabin feature from Pclass is that we don't know whether the missing variables are MCAR (Missing Completely at Random) or whether there is a relationship between the reason they are missing and the outcome (Survival).

Therefore, it would be more sensible to drop this variable and don't use in building our classifiers.

####Remove Cabin feature from all sets:

```{r}
training <- dplyr::select(training, - Cabin)
tune.test.set <- dplyr::select(tune.test.set, -Cabin)
testing <- dplyr::select(testing, -Cabin)
```

## Age feature: ~ 20% missing data

One intuitive imputation potential could be comparing the Age with SibSp feature:

```{r}
qplot(x = SibSp, y = Age, color = Survived, data = training)+
       theme_bw()
  
```
We notice that as the SibSp increases, the age group decreases. 
```{r}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ SibSp)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
```
Most of the Age distribution is having either no Spouses or siblings or only 1.  For the 1 siblings/spouses group the mean age is higher. In both cases Age is appromated by normal distribution. The problem with SibSp is that some of the factor levels are only present in the observations where Age is missing, making this predictor unbalanced across the missing and complete cases.

It would be also interesting to explore gender differences when considering Age:
```{r}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ Sex)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
```
How about the Age distribution in different passenger classes? (Pclass):

```{r}
ggplot(data = training, aes(x = Age, fill= Survived))+
        geom_histogram(bins = 40)+facet_grid(. ~ Pclass)+ scale_fill_manual(values = c("red","navy"))+
        theme_bw()
training %>% group_by(Pclass,Survived) %>% summarise(mean(Age,na.rm=T))
```
This is quite interesting! In all passenger classes, the mean age of the survived passengers is lower than those passed away. We also notice that the mean age decreases and the Class number increases, i.e: older passengers are in the better classes on average. Therefore, Pclass would also be include in the imputation model.

It would be therefore sensible to impute Age by random gaussian imputation using the mean and standard deviation of individual factor levels of these predictors. We will include Sex in this model to account for gender-specific differences in Age, as well as Pclass to account for the interesting seperation of Age by Passenger Class we noted above. In order to do this more rigorously, we can first fit a linear model with the existing Age , Sex  and Pclass  data:

##### Age ~  Sex + Pclass + e (random gaussian error)

```{r}
lmAge = lm(Age ~ Sex + Pclass, data = training, na.action = "na.omit")
summary(lmAge)
par(mfrow = c(2,2))
plot(lmAge)[1:4]
plot(lmAge)[2]
plot(lmAge)[3]
plot(lmAge)[4]
```
This model is just OK, but we don't need the perfect model for this type of imputation. Nice to see that both variance and normality assumptions of the model holds and all levels of the covariates have significant impact on the mean outcome in the presence of each other. This would give us a good estimation for the missing values of Age.

Just to check if our imputation model yields the similar distribution as the original age:
```{r}
complete.cases.Age <- training$Age[complete.cases(training$Age)]
after.imputation.Age <-c(complete.cases.Age,predict(lmAge, newdata = training[is.na(training$Age),]))
just.imputed.observations.Age <- after.imputation.Age[!complete.cases(training$Age)]

par(mfrow = c(1,3))
hist(complete.cases.Age,breaks = 20,col = "navy");
hist(after.imputation.Age,breaks = 20,col = "lightgreen");
hist(just.imputed.observations.Age,breaks = 20, col = "purple")

```
Therefore, our imputation performs a nice job 
Using the model to impute missing values of Age:

```{r}
training$Age[is.na(training$Age)] = predict(lmAge, newdata = training[is.na(training$Age),])
tune.test.set$Age[is.na(tune.test.set$Age)] = predict(lmAge, newdata = tune.test.set[is.na(tune.test.set$Age),])
testing$Age[is.na(testing$Age)] = predict(lmAge, newdata = testing[is.na(testing$Age),])

```
Note that we use the same model object we derived from the training data set to impute the missing values of Age for all data sets. This will prevent us from overfitting to the test data set.

Re-investigate the missing values:
```{r}
training.na <- as.data.frame(is.na(training));names(training.na) <- names(training)
apply(training.na,2,sum)

tune.testing.na <- as.data.frame(is.na(tune.test.set));names(tune.testing.na) <- names(tune.test.set)
apply(tune.testing.na ,2,sum)

testing.na <- as.data.frame(is.na(testing));names(testing.na) <- names(testing)
apply(testing.na ,2,sum)

```
It appears that only one case is left missing in each of the data sets. We will consider this as random missingness and remove in each data set:

```{r}
training <- training[complete.cases(training),]
tune.test.set <- tune.test.set[complete.cases(tune.test.set),]
testing <- testing[complete.cases(testing),]
```

Now we have completed processing the missing values.


#Feature Engineering:

## Generate Dummy Variables with Factor variables

Better to perform this step at the very end for all categorical variables.

```{r}
library(caret)

which(sapply(training[,-1],is.factor))
# There are 5 categorical predictors in out data set

factors.training <- which(sapply(training,is.factor))
factors.tune.test.set <- which(sapply(tune.test.set,is.factor))
factors.testing <- which(sapply(testing,is.factor))


dummies.training <- dummyVars(Survived ~ Pclass + Sex + SibSp + Parch + Embarked, data = training)

dummies.tune.test.set <- dummyVars(Survived ~ Pclass + Sex + SibSp + Parch + Embarked, data = tune.test.set)

dummies.testing <- dummyVars(PassengerId ~ Pclass + Sex + SibSp + Parch + Embarked, data = testing)


# Add the dummy variables to both training and test data sets, simultaneously removing the existing factor variables:
training <- cbind(training[,-factors.training[-1]], predict(dummies.training,newdata = training))
tune.test.set <- cbind(tune.test.set[,-factors.tune.test.set[-1]], predict(dummies.tune.test.set,newdata = tune.test.set))
testing <- cbind(testing[,-factors.testing], predict(dummies.testing,newdata = testing))
```

After doing that we notice that there are differences in the factor levels of certain variables are not present in the training set and test sets. These are present in Parch and SibSp features. The lower levels of these variables are conserved in all sets and they represent most of the data. At this stage we will only keep features that are present in all sets:

```{r}
training <- training[,names(training) %in% names(tune.test.set)]
tune.test.set <- tune.test.set[,names(tune.test.set) %in% names(training)]
identical(names(training),names(tune.test.set))
PassengerId <- testing$PassengerId
testing <- testing[,names(testing) %in% names(training)]
testing$PassengerId <- PassengerId
```

### Feature engineering with Name and Ticket variables

### Ticket feature:

It would be interesting to just convert the ticket feature to numeric and see if it has any seperation value:
```{r}

Ticket <- toupper(training$Ticket)
# Better to remove anything left of the last white space
w <- grep(" ", Ticket)
last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
        max(y[1])
})

Ticket[w] <- substring(Ticket[w],last.space+1)

Ticket <- gsub(" ","", Ticket)
Ticket <- gsub("[A-Z]","",Ticket)
Ticket <- gsub("\\.","",Ticket)
Ticket <- gsub("\\/","", Ticket)
Ticket <- as.numeric(Ticket)
Ticket[is.na(Ticket)] = 0
Ticket <- as.numeric(Ticket)


qplot(x=Ticket, y = Fare, data = training, color = Survived)+theme_bw()

```
We note that at least two groups of ticket numbers are associated with higher fares, and to some extent they seperate the outcome classes. Therefore, we will engineer this numeric feature and add into all sets, we will remove the original variable.

```{r}
# Feature Engineering function
num.Ticket <- function(x){
Ticket <- toupper(x$Ticket)
# Better to remove anything left of the last white space
w <- grep(" ", Ticket)
last.space<- sapply(gregexpr(" ",Ticket[w]), function(y){
        max(y[1])
})
Ticket[w] <- substring(Ticket[w],last.space+1)

Ticket <- gsub(" ","", Ticket)
Ticket <- gsub("[A-Z]","",Ticket)
Ticket <- gsub("\\.","",Ticket)
Ticket <- gsub("\\/","", Ticket)
Ticket <- as.numeric(Ticket)
Ticket <- as.numeric(Ticket)
return(Ticket)
}

training$num.Ticket <- num.Ticket(training)
tune.test.set$num.Ticket <- num.Ticket(tune.test.set)
testing$num.Ticket <- num.Ticket(testing)


sum(is.na(training$num.Ticket ))
sum(is.na(tune.test.set$num.Ticket ))
sum(is.na(testing$num.Ticket))
```
After this conversion, only 2 missing values were introduced to training and tune.testing data sets.

### Name feature:

When looking into the names of the passengers, we notice that at least we can attempt to extract a "title" feature from the name strings and explore its relationship with the outcome:

```{r}
Pass.Names <- training$Name

first.comma<- sapply(gregexpr(",",Pass.Names), function(y){
        y[1][1]
})
first.dot <- sapply(gregexpr("\\.",Pass.Names), function(y){
        y[1][1]
})

Titles <- substr(Pass.Names,first.comma+2,first.dot-1)

qplot(x = factor(Titles), y = Age ,color = Survived, data = training)
```

This feature can also be useful, therefore we will add into the data sets:

```{r}
# Feature engineering function:
Titles <- function(x){
      Pass.Names <- x$Name

first.comma<- sapply(gregexpr(",",Pass.Names), function(y){
        y[1][1]
})
first.dot <- sapply(gregexpr("\\.",Pass.Names), function(y){
        y[1][1]
})

Titles <- substr(Pass.Names,first.comma+2,first.dot-1)  
return(Titles)
        
}

training$Titles <- factor(Titles(training))
tune.test.set$Titles <- factor(Titles(tune.test.set))
testing$Titles <- factor(Titles(testing))

```
Next, we need to create dummy variables from each of these titles:

```{r}
factors.training <- which(sapply(training,is.factor))
factors.tune.test.set <- which(sapply(tune.test.set,is.factor))
factors.testing <- which(sapply(testing,is.factor))


dummies.training <- dummyVars(Survived ~ Titles, data = training)

dummies.tune.test.set <- dummyVars(Survived ~ Titles, data = tune.test.set)

dummies.testing <- dummyVars(PassengerId ~ Titles, data = testing)
```

# Add the dummy variables to both training and test data sets, simultaneously removing the existing factor variables:
```{r}
training <- cbind(training[,-factors.training[-1]], predict(dummies.training,newdata = training))
tune.test.set <- cbind(tune.test.set[,-factors.tune.test.set[-1]], predict(dummies.tune.test.set,newdata = tune.test.set))
testing <- cbind(testing[,-factors.testing], predict(dummies.testing,newdata = testing))

```

Not surprisingly ,after doing that we notice that there are differences in the factor levels of Titles feature. At this stage we will only keep features that are present in all sets:

```{r}
training <- training[,names(training) %in% names(tune.test.set)]
tune.test.set <- tune.test.set[,names(tune.test.set) %in% names(training)]

identical(names(training),names(tune.test.set))

PassengerId <- testing$PassengerId

testing <- testing[,names(testing) %in% names(training)]
testing$PassengerId <- PassengerId

```

We should also remove the original Ticket and Name features from all data sets since we already have the engineered versions included:

```{r}
training <- dplyr::select(training,-Ticket,-Name)
tune.test.set <- dplyr::select(tune.test.set ,-Ticket,-Name)
testing <- dplyr::select(testing,-Ticket,-Name)
```

Finally, since we had 2 missing values we generated due to the Ticket feature engineering, we need to get the complete cases in each of the data set:

```{r}
training <- training[complete.cases(training),]
tune.test.set <- tune.test.set[complete.cases(tune.test.set),]
testing <- testing[complete.cases(testing),]

testing <- data.frame(PassengerId = testing$PassengerId, testing[,-27])
```

This completes the feature engineering for all data sets.

## Collinearity, Near Zero Variance and Dimension Reduction

# Remove near-zero variance features:

Once completed the feature engineering, let's explore if there are near zero variance features in the training data set:

```{r}
nsv <- nearZeroVar(x = training, saveMetrics = TRUE)
sum(!nsv$nzv)
```
20 out of the 27 variables have non-zero variance and will be kept in the data sets.
```{r}
training <- training[,!nsv$nzv]
tune.test.set <- tune.test.set[,!nsv$nzv]
testing <- testing[,!nsv$nzv]
```

## Test for high correlation:

```{r}
M <- abs(cor(training[,-1])) # M is an absolute value correlation matrix representing the pairwise #correlations between all variables 
diag(M) <- 0 # We replace the diagonal values with zero (just because these are the correations with  #themselves we are not interested in capturing them).
which(M > 0.8, arr.ind = TRUE) # What are the highest correated variables?
unique(row.names(which(M > 0.8, arr.ind = TRUE)))

cor.variables <- training[,unique(row.names(which(M > 0.8, arr.ind = TRUE)))]
cor.variables$Survived <- training$Survived
```

## Principal components analysis

Next we will perform PCA to see if dimension reduction might help to reduce the highly correlated predictors in the data set:

```{r}
prePCA <- preProcess(cor.variables[,-6],method = "pca")
PCAcor <- predict(prePCA,cor.variables[,-6])
qplot(PCAcor$PC1,PCAcor$PC2, color = Survived, data = cor.variables)
qplot(PCAcor$PC3,PCAcor$PC2, color = Survived, data = cor.variables)
```

Since these are all binary variables, PCA did not furher help to reduce the dimensions.

We will use the remaining 20 features to train our classifiers.

## Training Classifiers

Due to the classification nature of the problem, we will perform tree-based prediction algorithms and support vector machines to perform predictions.

- Simple classification tree (method = "rpart") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RPART <- train(Survived ~ ., data = training,method = "rpart", trControl = trainControl(method = "cv", number = 10))
saveRDS(RPART,"RPART.rds") #Save model object for future loading if necessary
```

- Random Forest (method = "rf") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RF <- train(Survived ~ ., data = training,method = "rf", trControl = trainControl(method = "cv", number = 10)) 
saveRDS(RF,"RF.rds") #Save model object for future loading if necessary

```

- Boosted tree (method = "gbm") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
GBM <- train(Survived ~ ., data = training,method = "gbm", trControl = trainControl(method = "cv", number = 10), verbose = FALSE) 
saveRDS(GBM,"GBM.rds") #Save model object for future loading if necessary
```

- support vector machines with a radial kernel

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
SVM <- train(Survived ~ ., data = training,method = "svmRadial", trControl = trainControl(method = "cv", number = 10))
# Takes very long time to run!!
saveRDS(SVM,"SVM.rds")
```


# Summarizing the initial results from the individual classifiers
```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
# Using resamples function from the caret package to summarize the data

modelsummary <- resamples(list(RPART=RPART,RF=RF,GBM=GBM,SVM=SVM))

# In-sample accuracy values for each model
summary(modelsummary)$statistics$Accuracy

# If necessary we can also produce 'variable importance' plots to see which variables are making most contributions #to each model
dotPlot(varImp(RPART))
dotPlot(varImp(RF))
dotPlot(varImp(SVM))
dotPlot(varImp(GBM))
```

# Making predictions by using the classifiers and tune.test.set

```{r,results= 'markup', message=FALSE,warning=FALSE, cache=TRUE}

pred.tune.RPART <- predict(RPART,newdata = tune.test.set)
pred.tune.RF <- predict(RF,newdata = tune.test.set)
pred.tune.SVM <- predict(SVM,newdata = tune.test.set)
pred.tune.GBM <- predict(GBM,newdata = tune.test.set)

# Collect accuracy values in a data.frame:

tune.testing.predictions <-data.frame(pred.tune.RPART,pred.tune.RF,pred.tune.SVM,pred.tune.GBM)

tune.testing.accuracy <- apply(tune.testing.predictions, 2, function(x){
        temp=confusionMatrix(x,tune.test.set$Survived)$overall[1]
        names(temp) <- names(x)
        return(temp)
})
tune.testing.accuracy
```

Based on the prediction in the tune.test.set, Random Forest classification tree gave the highest out of the box accuracy. The accuracy is ~ 83.3 %. 


___

Conclude the report here, the rest is just further model tuning, which did not further improve the accuracy. In the future, I may try to perform model stacking/ensembling, also try other types of feature engineering as I learn different approaches. 
___

# Tuning with trainControl()

- Random Forest (method = "rf") with bootstrap632

**Note that using boot632 method for training increased in sample accuracy to 88%**

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RF.tune <- train(Survived ~ ., data = training,method = "rf", trControl = trainControl(method = "boot632", number = 10) )
saveRDS(RF.tune,"RFtune.rds") #Save model object for future loading if necessary

```

**Next evaluate LOOCV method. This take a long time to train with RF**




### What if we had used dimension reduction?

```{r}
prePCA.training <- preProcess(training[,-1],method = "pca")
PCA.training <- predict(prePCA.training,training[,-1])
PCA.training$Survived <- training$Survived
qplot(PCA.training$PC1,PCA.training$PC2, color = Survived, data = PCA.training)
qplot(PCA.training$PC2,PCA.training$PC3, color = Survived, data = PCA.training)
qplot(PCA.training$PC1,PCA.training$PC3, color = Survived, data = PCA.training)

```
The seperation is indeed quite good.

Let's try to fit models by using the Principal components, instead of the original data:



- Simple classification tree (method = "rpart") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RPART.PCA <- train(Survived ~ ., data = PCA.training,method = "rpart", trControl = trainControl(method = "cv", number = 10))
saveRDS(RPART.PCA,"RPART.PCA.rds") #Save model object for future loading if necessary
```

- Random Forest (method = "rf") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RF.PCA <- train(Survived ~ ., data = PCA.training,method = "rf", trControl = trainControl(method = "cv", number = 10)) 
saveRDS(RF.PCA,"RF.PCA.rds") #Save model object for future loading if necessary

```

- Boosted tree (method = "gbm") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
GBM.PCA <- train(Survived ~ ., data = PCA.training,method = "gbm", trControl = trainControl(method = "cv", number = 10), verbose = FALSE) 
saveRDS(GBM.PCA,"GBM.PCA.rds") #Save model object for future loading if necessary
```

- support vector machines with a radial kernel

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
SVM.PCA <- train(Survived ~ ., data = PCA.training,method = "svmRadial", trControl = trainControl(method = "cv", number = 10))
# Takes very long time to run!!
saveRDS(SVM.PCA,"SVM.PCA.rds")
```

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
# Using resamples function from the caret package to summarize the data

modelsummary.PCA <- resamples(list(RPART=RPART.PCA,RF=RF.PCA,GBM=GBM.PCA,SVM=SVM.PCA))

# In-sample accuracy values for each model
summary(modelsummary.PCA)$statistics$Accuracy

# If necessary we can also produce 'variable importance' plots to see which variables are making most contributions #to each model
dotPlot(varImp(RPART.PCA))
dotPlot(varImp(RF.PCA))
dotPlot(varImp(SVM.PCA))
dotPlot(varImp(GBM.PCA))
```
Not so much improvement of in sample accuracy. 

What happens if we had only used the first 3 PCs?

- Simple classification tree (method = "rpart") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RPART.PCA3 <- train(Survived ~ PC1 + PC2 + PC3 , data = PCA.training,method = "rpart", trControl = trainControl(method = "cv", number = 10))
saveRDS(RPART.PCA3,"RPART.PCA3.rds") #Save model object for future loading if necessary
```

- Random forest with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RF.PCA3 <- train(Survived ~ PC1 + PC2 + PC3 , data = PCA.training,method = "rf", trControl = trainControl(method = "cv", number = 10))
saveRDS(RF.PCA3,"RF.PCA3.rds") #Save model object for future loading if necessary
```
Not improved,either.

# Stepping back

What if we didn't perform extensive feature engineering?:

Being simplistic, what if we just performed a complete case analysis with few seemingly important variables?

Going back to our EDA, we noted that the features:

- Sex
- Pclass
- Age
- Parch
- SibSp
- Fare

showed some seperation. Let's just focus on these features and perform a complete case analysis:

```{r}
simplfied.features <- "Sex|Pclass|Age|Parch|SibSp|Fare|Survived"
training.simplified <- training[,grepl(simplfied.features,names(training))]
tune.test.set.simplified <- tune.test.set[,grepl(simplfied.features,names(tune.test.set))]
```

- Simple classification tree (method = "rpart") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RPART.simp <- train(Survived ~ . , data = training.simplified,method = "rpart", trControl = trainControl(method = "cv", number = 10))
```

- SVM -linear with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
SVM.simp <- train(Survived ~ . , data = training.simplified,method = "svmLinear3", trControl = trainControl(method = "cv", number = 10))
```

- Gradient boosted tree with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
GBM.simp <- train(Survived ~ . , data = training.simplified,method = "gbm", trControl = trainControl(method = "cv", number = 1000))
```

# Stepping back again

What if we modeled the factor features such as Pclass and Parch as continuous variables?:

Load the data sets:

```{r}
training <- read.csv("train.csv", stringsAsFactors = FALSE, na.strings = "")
testing <- read.csv("test.csv", stringsAsFactors = FALSE,na.strings = "")
```

Partition the training set into:

- training: train different models
- tune.test.set: initial out of the box performance

```{r,results='markup'}
library(caret)
set.seed(1234)
InTrain <- createDataPartition(y=training$Survived,p = 0.7,list = FALSE)

tune.test.set <- training[-InTrain,]
training <- training[InTrain,]

```

Going back to our EDA, we noted that these features showed some seperation:

- Sex
- Pclass
- Age
- Parch
- SibSp
- Fare

Let's limit our training set to these variables for now and get some in sample prediction accuracy. In this case we are modeling all features except Sex as continuous variable:

```{r}
library(dplyr)
Survived <- factor(training$Survived)
training <- dplyr::select(training, Sex, Pclass,Age,Parch,SibSp,Fare)
training$male <- ifelse(training$Sex == "male",1,0)
training <- dplyr::select(training, -Sex) %>% mutate(Survived = Survived)

```

###Impute Age feature:


```{r}
lmAge = lm(Age ~ male + Pclass, data = training, na.action = "na.omit")
summary(lmAge)
par(mfrow = c(2,2))
plot(lmAge)[1]
plot(lmAge)[2]
plot(lmAge)[3]
plot(lmAge)[4]
```

Just to check if our imputation model yields the similar distribution as the original age:
```{r}
complete.cases.Age <- training$Age[complete.cases(training$Age)]
after.imputation.Age <-c(complete.cases.Age,predict(lmAge, newdata = training[is.na(training$Age),]))
just.imputed.observations.Age <- after.imputation.Age[!complete.cases(training$Age)]

par(mfrow = c(1,3))
hist(complete.cases.Age,breaks = 20,col = "navy");
hist(after.imputation.Age,breaks = 20,col = "lightgreen");
hist(just.imputed.observations.Age,breaks = 20, col = "purple")

```
Therefore, our imputation performs a nice job 

Using the model to impute missing values of Age:

```{r}
training$Age[is.na(training$Age)] = predict(lmAge, newdata = training[is.na(training$Age),])


```


- Simple classification tree (method = "rpart") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RPART.cont <- train(Survived ~ . , data = training,method = "rpart", trControl = trainControl(method = "cv", number = 10))
```

- Gradient boosted tree with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
GBM.cont <- train(Survived ~ . , data = training,method = "gbm", trControl = trainControl(method = "cv", number = 10), verbose = FALSE)
```

- Random Forest classifier (method = "rf") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
RF.cont <- train(Survived ~ . , data = training,method = "rf", trControl = trainControl(method = "cv", number = 100))
```


- Logistic regression classifier (method = "glm") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
GLM.cont <- train(Survived ~ . , data = training,method = "glm", trControl = trainControl(method = "cv", number = 100))
```

- Linear Discriminant Analysis (method = "lda") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
LDA.cont <- train(Survived ~ . , data = training,method = "lda", trControl = trainControl(method = "cv", number = 100))
```

- Support Vector Machines (method = "svmRadial") with cross validation

```{r,results='markup', message=FALSE,warning=FALSE, cache=TRUE}
set.seed(125745)
SVM.cont <- train(Survived ~ . , data = training,method = "svmRadial", trControl = trainControl(method = "cv", number = 150))
```
This approach also yields ~ 81% in sample accuracy as maximum. 
